{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------PART_J BEGINs--------\n",
      "min_df is: 2\n",
      "Multiclass MultiNomialNB result:\n",
      "confusion matrix:\n",
      "[[319  19  48   6]\n",
      " [106 216  54   9]\n",
      " [ 37   7 340   6]\n",
      " [  3   0   5 390]]\n",
      "accuracy:\n",
      "0.808306709265\n",
      "recall:\n",
      "0.808306709265\n",
      "precision:\n",
      "0.808306709265\n",
      "multiclass SVC NMF one vs one result:\n",
      "confusion matrix:\n",
      "[[327  45  20   0]\n",
      " [ 73 292  19   1]\n",
      " [ 36  16 337   1]\n",
      " [ 13   3   4 378]]\n",
      "accuracy:\n",
      "0.852396166134\n",
      "recall:\n",
      "0.852396166134\n",
      "precision:\n",
      "0.852396166134\n",
      "multiclass SVC LSI one vs one result:\n",
      "confusion matrix:\n",
      "[[324  45  21   2]\n",
      " [ 40 322  22   1]\n",
      " [ 24  13 351   2]\n",
      " [  4   2   0 392]]\n",
      "accuracy:\n",
      "0.887539936102\n",
      "recall:\n",
      "0.887539936102\n",
      "precision:\n",
      "0.887539936102\n",
      "multiclass SVC NMF one vs rest result:\n",
      "confusion matrix:\n",
      "[[312  45  31   4]\n",
      " [ 54 300  26   5]\n",
      " [ 20  12 354   4]\n",
      " [  2   2   2 392]]\n",
      "accuracy:\n",
      "0.867731629393\n",
      "recall:\n",
      "0.867731629393\n",
      "precision:\n",
      "0.867731629393\n",
      "multiclass SVC LSI one vs rest result:\n",
      "confusion matrix:\n",
      "[[320  49  23   0]\n",
      " [ 33 323  29   0]\n",
      " [ 16  16 355   3]\n",
      " [  3   1   1 393]]\n",
      "accuracy:\n",
      "0.888817891374\n",
      "recall:\n",
      "0.888817891374\n",
      "precision:\n",
      "0.888817891374\n",
      "--------PART_J BEGINs--------\n"
     ]
    }
   ],
   "source": [
    "#import functions\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from pickle import dump\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#parameters switch for each parts\n",
    "PART_A = False\n",
    "PART_B = False\n",
    "PART_E = False\n",
    "PART_F = False\n",
    "PART_G = False\n",
    "PART_H = False\n",
    "PART_I = False\n",
    "PART_J = True\n",
    "#change min_dfs\n",
    "min_dfs = 2#2 or 5\n",
    "\n",
    "#if need download, decomment these lines\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download()\n",
    "#nltk.download('stopwords' )\n",
    "\n",
    "#For all parts instead of multiclass and part C\n",
    "comp_categories = [ 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n",
    "rec_categories = ['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "combine_train = fetch_20newsgroups(subset='train', categories=comp_categories+rec_categories, shuffle=True, random_state=42,)\n",
    "combine_test = fetch_20newsgroups(subset='test', categories=comp_categories+rec_categories, shuffle=True, random_state=42,)\n",
    "\n",
    "#for complete stop_words and build analyzer\n",
    "stop_words_skt = text.ENGLISH_STOP_WORDS\n",
    "stop_words_en = stopwords.words('english')\n",
    "combined_stopwords = set.union(set(stop_words_en),set(punctuation),set(stop_words_skt))\n",
    "\n",
    "#analyzer for part B\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "wnl = nltk.wordnet.WordNetLemmatizer()\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "def stem_rmv_punc(doc):\n",
    "    return (word for word in lemmatize_sent(analyzer(doc)) if word not in combined_stopwords and not word.isdigit())\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "def lemmatize_sent(list_word): \n",
    "    # Text input is string, returns array of lowercased strings(words).\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(list_word)]\n",
    "\n",
    "#plotting function\n",
    "%matplotlib inline\n",
    "def plot_roc(fpr, tpr):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "\n",
    "    ax.plot(fpr, tpr, lw=2, label= 'area under curve = %0.4f' % roc_auc)\n",
    "\n",
    "    ax.grid(color='0.7', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate',fontsize=15)\n",
    "    ax.set_ylabel('True Positive Rate',fontsize=15)\n",
    "\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    for label in ax.get_xticklabels()+ax.get_yticklabels():\n",
    "        label.set_fontsize(15)\n",
    "\n",
    "#predict and plotting for most part\n",
    "def fit_predict_and_plot_roc(pipe, train_data, train_label, test_data, test_label):\n",
    "    pipe.fit(train_data, train_label)\n",
    "    predicted_l = pipe.predict(test_data)\n",
    "    #confusion matrix\n",
    "    confusion_l = confusion_matrix(test_label, predicted_l)\n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_l)\n",
    "    print(\"accuracy:\")\n",
    "    acc = accuracy_score(test_label, predicted_l)\n",
    "    print(acc)\n",
    "    print(\"recall:\")\n",
    "    rec = recall_score(test_label, predicted_l)\n",
    "    print(rec)\n",
    "    print(\"precision:\")\n",
    "    pre = precision_score(test_label, predicted_l)\n",
    "    print(pre)\n",
    "    prob_score = pipe.predict_proba(test_data)\n",
    "    fpr, tpr, _ = roc_curve(test_label, prob_score[:,1])   \n",
    "    plot_roc(fpr, tpr)\n",
    "    return pipe\n",
    "\n",
    "#predict for multiclass part\n",
    "def fit_predict_and_plot_roc_multi(pipe, train_data, train_label, test_data, test_label):\n",
    "    pipe.fit(train_data, train_label)\n",
    "    predicted_l = pipe.predict(test_data)\n",
    "    #confusion matrix\n",
    "    confusion_l = confusion_matrix(test_label, predicted_l)\n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_l)\n",
    "    print(\"accuracy:\")\n",
    "    acc = accuracy_score(test_label, predicted_l)\n",
    "    print(acc)\n",
    "    print(\"recall:\")\n",
    "    rec = recall_score(test_label, predicted_l, average = 'micro')\n",
    "    print(rec)\n",
    "    print(\"precision:\")\n",
    "    pre = precision_score(test_label, predicted_l, average = 'micro')\n",
    "    print(pre)\n",
    "    prob_score = pipe.predict_proba(test_data)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "#predict and report only accuracy for k-fold\n",
    "def fit_predict_acc(pipe, train_data, train_label, test_data, test_label):\n",
    "    pipe.fit(train_data, train_label)\n",
    "    predicted_l = pipe.predict(test_data)\n",
    "    acc = accuracy_score(test_label, predicted_l)\n",
    "    return acc\n",
    "\n",
    "class SparseToDenseArray(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        if hasattr(X, 'toarray'):\n",
    "            return X.toarray()\n",
    "        return X\n",
    "\n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "\n",
    "#usual count_vect\n",
    "count_vect = CountVectorizer(min_df=min_dfs,stop_words = 'english')#use CountVectorizer(min_df=min_dfs,analyzer=stem_rmv_punc)\n",
    "#part A\n",
    "if PART_A:\n",
    "    print(\"--------PART_A BEGINs--------\")\n",
    "    histy = []\n",
    "    for i in range(8):\n",
    "        histy.append(0)\n",
    "    \n",
    "    for i in range(len(combine_train.target)):\n",
    "        histy[combine_train.target[i]] = histy[combine_train.target[i]] + 1\n",
    "    print (\"histogram data:\")\n",
    "    print (histy)\n",
    "    x = [0,1,2,3,4,5,6,7]\n",
    "    plt.bar(x, histy)\n",
    "    plt.title('Histogram of the number of training docs per class')\n",
    "    plt.show()\n",
    "    for i in range(8):\n",
    "        print(\"Topic %d is %s\" %(i, combine_train.target_names[i]))\n",
    "        \n",
    "    print(\"--------PART_A ENDs--------\")\n",
    "\n",
    "#part B  \n",
    "if PART_B:\n",
    "    print(\"--------PART_B BEGINs--------\")\n",
    "    count_vect1 = CountVectorizer(min_df=min_dfs, analyzer=stem_rmv_punc)    \n",
    "    count_train1_count = count_vect1.fit_transform(combine_train.data)\n",
    "    count_trina1_tfidf = tfidf_transformer.fit_transform(count_train1_count)\n",
    "    print(\"min_df is: %d\"%min_dfs)\n",
    "    print(\"shape of the final matrix:\")\n",
    "    print(count_trina1_tfidf.shape)\n",
    "    print(\"--------PART_B ENDs--------\")\n",
    "    \n",
    "#part C in another program\n",
    "\n",
    "# relabel datas for all the remaining part except multiclass\n",
    "for i in range(len(combine_train.target)):\n",
    "    if(combine_train.target[i] == 0 or combine_train.target[i] == 1 or combine_train.target[i] == 2 or combine_train.target[i] == 3):\n",
    "        combine_train.target[i] = 0\n",
    "    if(combine_train.target[i] >= 4):\n",
    "        combine_train.target[i] = 1\n",
    "\n",
    "for i in range(len(combine_test.target)):\n",
    "    if(combine_test.target[i] <= 3):\n",
    "        combine_test.target[i] = 0\n",
    "    if(combine_test.target[i] >= 4):\n",
    "        combine_test.target[i] = 1\n",
    "    \n",
    "for i in range(4):\n",
    "    combine_train.target_names[i] = 'Computer Technology'\n",
    "    combine_test.target_names[i] = 'Computer Technology'\n",
    "    combine_train.target_names[i + 4] = 'Recreational Activity'\n",
    "    combine_test.target_names[i + 4] = 'Recreational Activity'\n",
    "\n",
    "#part E\n",
    "#change the pipeline manually for different tasks (LSI and NMF)\n",
    "if PART_E :\n",
    "    #Hard SVC LSI\n",
    "    print(\"--------PART_E BEGINs--------\")\n",
    "    pipeline1 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', TruncatedSVD(n_components=50, random_state=0)),\n",
    "    ('clf',SVC(C = 1000, probability=True)),\n",
    "    ])\n",
    "    #Soft SVC LSI\n",
    "    pipeline2 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', TruncatedSVD(n_components=50, random_state=0)),\n",
    "    ('clf',SVC(C = 0.001, probability=True)),\n",
    "    ])\n",
    "    #Hard SVC NMF\n",
    "    pipeline3 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', NMF(n_components=50, init='random', random_state=0)),\n",
    "    ('clf',SVC(C = 1000, probability=True)),\n",
    "    ])\n",
    "    #soft SVC NMF\n",
    "    pipeline4 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', NMF(n_components=50, init='random', random_state=0)),\n",
    "    ('clf',SVC(C = 0.001, probability=True)),\n",
    "    ])\n",
    "    print(\"min_df is: %d\"%min_dfs)\n",
    "    fit_predict_and_plot_roc(pipeline4, combine_train.data, combine_train.target, combine_test.data, combine_test.target)\n",
    "    print(\"--------PART_E ENDs--------\")\n",
    "\n",
    "    \n",
    "#PART_F\n",
    "#manually change the pipeline for LSI and NMF\n",
    "if PART_F :\n",
    "    print(\"--------PART_F BEGINs--------\")\n",
    "    print(\"min_df is: %d\"%min_dfs)\n",
    "    sum = 0\n",
    "    kf = KFold(n_splits = 5)\n",
    "    kf.get_n_splits(combine_train.data)\n",
    "    for i in range(-3, 4):\n",
    "        newC = math.pow(10, i)\n",
    "        sum = 0\n",
    "        #pipeline for LSI\n",
    "        pipeline5 = Pipeline([\n",
    "            ('vect', count_vect),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('reduce_dim', TruncatedSVD(n_components=50, random_state=0)),\n",
    "            ('toarr', SparseToDenseArray()),\n",
    "            ('clf',SVC(C = newC, probability=True)),\n",
    "            ])\n",
    "        #pipeline for NMF\n",
    "        pipeline6 = Pipeline([\n",
    "            ('vect', count_vect),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('reduce_dim', NMF(n_components=50, init='random', random_state=0)),\n",
    "            ('toarr', SparseToDenseArray()),\n",
    "            ('clf',SVC(C = newC, probability=True)),\n",
    "            ])\n",
    "        for train, test in kf.split(combine_train.data):  \n",
    "            train_kf_data = np.array(combine_train.data)[train]\n",
    "            test_kf_data = np.array(combine_train.data)[test]\n",
    "            train_kf_label = np.array(combine_train.target)[train]\n",
    "            test_kf_label = np.array(combine_train.target)[test]    \n",
    "            sum = sum+fit_predict_acc(pipeline5, train_kf_data, train_kf_label, test_kf_data, test_kf_label) #change the pipeline for different task\n",
    "        average_acc = sum/5\n",
    "        print(\"average_acc for C = %f:\"%newC)\n",
    "        print(average_acc)\n",
    "    print(\"--------PART_F ENDs--------\")\n",
    "    \n",
    "#PART_G    \n",
    "#manually change the class_prior\n",
    "if PART_G :\n",
    "    print(\"--------PART_G BEGINs--------\")\n",
    "    print(\"min_df is: %d\"%min_dfs)\n",
    "    pipeline7 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', NMF(n_components=50, init='random', random_state=0)),\n",
    "    ('toarr', SparseToDenseArray()),\n",
    "    ('clf',MultinomialNB(class_prior=[.4, .6])),#change the threshold here\n",
    "    ])\n",
    "    fit_predict_and_plot_roc(pipeline7, combine_train.data, combine_train.target, combine_test.data, combine_test.target)\n",
    "    print(\"--------PART_G ENDs--------\")\n",
    "    \n",
    "#PART_H\n",
    "#manually change the weight and pipelines for LSI and NMF\n",
    "if PART_H :\n",
    "    print(\"--------PART_H BEGINs--------\")\n",
    "    print(\"min_df is: %d\"%min_dfs)\n",
    "    # LSI logistics regression \n",
    "    pipeline8 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', TruncatedSVD(n_components=50, random_state=0)),\n",
    "    ('toarr', SparseToDenseArray()),\n",
    "    ('clf',LogisticRegression(class_weight = {0:0.8, 1:0.2})), #change the weight for now is 4 : 1\n",
    "    ])\n",
    "\n",
    "    # NMF logistics regression \n",
    "    pipeline9 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', NMF(n_components=50, init='random', random_state=0)),\n",
    "    ('toarr', SparseToDenseArray()),\n",
    "    ('clf',LogisticRegression(class_weight = {0:0.8, 1:0.2})), #change the weight for now is 4 : 1\n",
    "    ])\n",
    "\n",
    "    fit_predict_and_plot_roc(pipeline8, combine_train.data, combine_train.target, combine_test.data, combine_test.target)\n",
    "    print(\"--------PART_H ENDs--------\")\n",
    "        \n",
    "#PART_I        \n",
    "#manually change the pipeline and penalty for different task\n",
    "if PART_I :\n",
    "    print(\"--------PART_I BEGINs--------\")\n",
    "    for i in range(-2, 3, 2): #sweep from 0.01 to 100\n",
    "        print(\"min_df is: %d\"%min_dfs)\n",
    "        newC = math.pow(10, i)\n",
    "        # LSI logistics regression \n",
    "        pipeline10 = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('reduce_dim',TruncatedSVD(n_components=50, random_state=0)),\n",
    "        ('toarr', SparseToDenseArray()),\n",
    "        ('clf',LogisticRegression(penalty = 'l1', C = newC)), #change to be l1 or l2\n",
    "        ])\n",
    "        # NMF logistics regression\n",
    "        pipeline11 = Pipeline([\n",
    "        ('vect', count_vect),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('reduce_dim',NMF(n_components=50, init='random', random_state=0)),\n",
    "        ('toarr', SparseToDenseArray()),\n",
    "        ('clf',LogisticRegression(penalty = 'l1', C = newC)), #change to be l1 or l2\n",
    "        ])\n",
    "        acc = fit_predict_acc(pipeline10, combine_train.data, combine_train.target, combine_test.data, combine_test.target)\n",
    "        print(\"accuracy for C = %f:\"%newC)\n",
    "        print(acc)\n",
    "    print(\"--------PART_I ENDs--------\")   \n",
    "        \n",
    "#multiclass PART_J\n",
    "if PART_J :\n",
    "    print(\"--------PART_J BEGINs--------\")\n",
    "    print(\"min_df is: %d\"%min_dfs)\n",
    "    multi_categories = [ 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'misc.forsale', 'soc.religion.christian']\n",
    "    multi_train = fetch_20newsgroups(subset='train', categories=multi_categories, shuffle=True, random_state=42,)\n",
    "    multi_test = fetch_20newsgroups(subset='test', categories=multi_categories, shuffle=True, random_state=42,)\n",
    "    \n",
    "    #multiclass NB pipeline\n",
    "    pipeline12 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', NMF(n_components=50, init='random', random_state=0)),\n",
    "    ('toarr', SparseToDenseArray()),\n",
    "    ('clf',MultinomialNB()),\n",
    "    ])\n",
    "    \n",
    "    #multiclass SVC NMF one vs one pipeline\n",
    "    pipeline13 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', NMF(n_components=50, init='random', random_state=0)),\n",
    "    ('toarr', SparseToDenseArray()),\n",
    "    ('clf',SVC(C = 1000, probability = True)),\n",
    "    ])\n",
    "    \n",
    "    #multiclass SVC LSI one vs one pipeline\n",
    "    pipeline14 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', TruncatedSVD(n_components=50, random_state=0)),\n",
    "    ('toarr', SparseToDenseArray()),\n",
    "    ('clf',SVC(C = 1000, probability = True)),\n",
    "    ])\n",
    "    \n",
    "    #multiclass SVC NMF one vs rest pipeline\n",
    "    pipeline15 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', NMF(n_components=50, init='random', random_state=0)),\n",
    "    ('toarr', SparseToDenseArray()),\n",
    "    ('clf',OneVsRestClassifier(SVC(C = 1000, probability = True))),\n",
    "    ])\n",
    "    \n",
    "    #multiclass SVC LSI one vs rest pipeline\n",
    "    pipeline16 = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', TruncatedSVD(n_components=50, random_state=0)),\n",
    "    ('toarr', SparseToDenseArray()),\n",
    "    ('clf',OneVsRestClassifier(SVC(C = 1000, probability = True))),\n",
    "    ])\n",
    "    \n",
    "    print(\"Multiclass MultiNomialNB result:\")\n",
    "    fit_predict_and_plot_roc_multi(pipeline12, multi_train.data, multi_train.target, multi_test.data, multi_test.target)\n",
    "    print(\"multiclass SVC NMF one vs one result:\")\n",
    "    fit_predict_and_plot_roc_multi(pipeline13, multi_train.data, multi_train.target, multi_test.data, multi_test.target)\n",
    "    print(\"multiclass SVC LSI one vs one result:\")\n",
    "    fit_predict_and_plot_roc_multi(pipeline14, multi_train.data, multi_train.target, multi_test.data, multi_test.target)\n",
    "    print(\"multiclass SVC NMF one vs rest result:\")\n",
    "    fit_predict_and_plot_roc_multi(pipeline15, multi_train.data, multi_train.target, multi_test.data, multi_test.target)\n",
    "    print(\"multiclass SVC LSI one vs rest result:\")\n",
    "    fit_predict_and_plot_roc_multi(pipeline16, multi_train.data, multi_train.target, multi_test.data, multi_test.target)\n",
    "    \n",
    "    print(\"--------PART_J ENDs--------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
